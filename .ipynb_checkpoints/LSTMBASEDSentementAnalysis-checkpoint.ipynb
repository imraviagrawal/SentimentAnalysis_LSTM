{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the IMDB large movie review corpus from the class webpage to a file location on your computer\n",
    "\n",
    "PATH_TO_DATA = '../large_movie_review_dataset'  # set this variable to point to the location of the IMDB corpus on your computer\n",
    "POS_LABEL = 'pos'\n",
    "NEG_LABEL = 'neg'\n",
    "TRAIN_DIR = os.path.join(PATH_TO_DATA, \"train\")\n",
    "TEST_DIR = os.path.join(PATH_TO_DATA, \"test\")\n",
    "\n",
    "for label in [POS_LABEL, NEG_LABEL]:\n",
    "    if len(os.listdir(TRAIN_DIR + \"/\" + label)) == 12500:\n",
    "        print \"Great! You have 12500 {} reviews in {}\".format(label, TRAIN_DIR + \"/\" + label)\n",
    "    else:\n",
    "        print \"Oh no! Something is wrong. Check your code which loads the reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_doc_and_more(doc):\n",
    "    bow = defaultdict(float)\n",
    "    \n",
    "    # Converting into lower case text\n",
    "    doc_lower = doc.lower() \n",
    "    \n",
    "    # removing pucntuations (\"..\", \".\", \",\")\n",
    "    doc_wo_punc = re.sub(r'(\\.+$|\\?+$|\\,|\\'|\\.{2,}|<br *(/>)?)|-',\"\",doc_lower)\n",
    "    #doc_wo_punc = re.sub(r\"[^A-Za-z0-9 ]+\",\"\", doc_lower)\n",
    "    # removing two spaces \n",
    "    text_with_one_space = re.sub(r'[ ]{2,}',\" \", doc_wo_punc).split()\n",
    "    \n",
    "    # removing stop words like \"and\", \"the\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in text_with_one_space if not w in stop_words]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print tokenize_doc_and_more(\"I was looking forward to The Guardian, but when I walked into the theater I wasn't really in the mood for it at that particular time. It's kind of like the Olive Garden - I like it, but I have to be in the right mindset to thoroughly enjoy it.<br /><br />I'm not exactly sure what was dampening my spirit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading dataset and making list of list for positive and negative training examples.\n",
    "positiveSentence = []\n",
    "negativeSentence = []\n",
    "numberofWords = []\n",
    "for label in [POS_LABEL, NEG_LABEL]:\n",
    "    for directory in [TRAIN_DIR, TEST_DIR]:\n",
    "        for fn in glob.glob(directory + \"/\" + label + \"/*txt\"):\n",
    "            ## Implement me!\n",
    "            temp = tokenize_doc_and_more(open(fn).read())\n",
    "            numberofWords.append(len(temp))\n",
    "            if label == POS_LABEL:\n",
    "                positiveSentence.append(temp)\n",
    "            else:\n",
    "                negativeSentence.append(temp)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check the loaded data in the positive and negative sentence\n",
    "print \"The number of positive examples \" + str(len(positiveSentence))\n",
    "print \"The number of negative examples \" + str(len(negativeSentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Printing the number of words in all the sentences\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numberofWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum length of words is about 600, but most sentence are covered in 250 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxSeqLength = 250\n",
    "batchSize = 1\n",
    "numDimensions = 50\n",
    "numFiles= 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "# Loading the word vector \n",
    "filename = 'glove.6B.50d.txt'\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "\n",
    "# Glove Vector location\n",
    "fileName = \"../glove.6B/glove.6B.50d.txt\"\n",
    "wordList, embdd = loadGloVe(fileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordVector = np.asarray(embdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wordVector = np.load('wordVectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "# Number of words in the the word vector \n",
    "print len(list(wordVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "firstfile = np.zeros((maxSeqLength), dtype=\"int32\")\n",
    "indexCounter = 0\n",
    "for word in positiveSentence[0]:\n",
    "    try:\n",
    "        firstfile[indexCounter] = wordList.index(word)\n",
    "    except:\n",
    "        firstfile[indexCounter] = 399999\n",
    "    indexCounter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Id Matrix Commented because of computation of this code\n",
    "\n",
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "fileCounter = 0\n",
    "for sentence in positiveSentence:\n",
    "    indexCounter = 0 \n",
    "    for word in sentence:\n",
    "        try:\n",
    "            ids[fileCounter][indexCounter] = wordList.index(word)\n",
    "        except ValueError:\n",
    "            ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "        indexCounter = indexCounter + 1\n",
    "        if indexCounter >= maxSeqLength:\n",
    "            break\n",
    "    fileCounter = fileCounter + 1 \n",
    "\n",
    "for sentence in negativeSentence:\n",
    "    indexCounter = 0 \n",
    "    for word in sentence:\n",
    "        try:\n",
    "            ids[fileCounter][indexCounter] = wordList.index(word)\n",
    "        except ValueError:\n",
    "            ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "        indexCounter = indexCounter + 1\n",
    "        if indexCounter >= maxSeqLength:\n",
    "            break\n",
    "    fileCounter = fileCounter + 1\n",
    "np.save('idsMatrix', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading the id matrix\n",
    "ids = np.load(\"idsMatrix.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.44009995  0.48879001  0.30649999 ..., -0.47185999 -0.28049999  0.1142    ]\n",
      " [-0.65758002  1.25020003  0.19081999 ..., -0.45636001 -0.19089     0.099351  ]\n",
      " [ 0.61849999  0.64253998 -0.46551999 ..., -0.27557001  0.30899     0.48497   ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "idsTensor = tf.constant(ids[0])\n",
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVector,ids[0]).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
